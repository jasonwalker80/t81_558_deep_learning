{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T81-558: Applications of Deep Neural Networks\n",
    "* Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), School of Engineering and Applied Science, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n",
    "* For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/).\n",
    "\n",
    "**Module 7 Assignment: Computer Vision Neural Network**\n",
    "\n",
    "**Student Name: Jason Walker**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment Instructions\n",
    "\n",
    "For this assignment you will use the MINST dataset to train and predict several handwritten digits.  For the creation and training of the neural network, you can [copy the code](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class7_cnn.ipynb) from class for much of this assignment.  Use the provided code to download the training and testing data for the handwritten digits and predict the following **testing set** digits (10,20,30,40,50):\n",
    "\n",
    "* Index 10: which is the digit '0'\n",
    "* Index 20: which is the digit '9'\n",
    "* Index 30: which is the digit '3'\n",
    "* Index 40: which is the digit '1'\n",
    "* Index 50: which is the digit '6'\n",
    "\n",
    "You will predict these 5 digits. You will submit these perdictions to the submit function. See [Assignment #1]() for details on how to submit an assignment or check that one was submitted. Your submitted dataframe should have 5 data rows (one offor each of the indexes).  You should have the columns **id**, **pred**, and **y**.  The column **id** specifies which index the prediction is for (10, 20, 30, 40, 50).  The column **pred** is your prediction (0-9).  The column **y** is the actual digit for that index (0-9).  Your sumission will only have 5 rows and might look something like the following:\n",
    "\n",
    "id | pred | y\n",
    "---|---|---\n",
    "10 | 0 | 0\n",
    "20 | 9 | 9\n",
    "30 | 3 | 3\n",
    "40 | 1 | 1\n",
    "50 | 6 | 6\n",
    "\n",
    "**Important suggestion** You will likely see training times upwards of an hour.  Place your training code in a different cell than your code to get predictions and submit.  Otherwise you will have to retrain for every test of your network, and that will take forever!  You can also save your trained neural network to a binary file and reload it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpful Functions\n",
    "\n",
    "You will see these at the top of every module and assignment.  These are simply a set of reusable functions that we will make use of.  Each of them will be explained as the semester progresses.  They are explained in greater detail as the course progresses.  Class 4 contains a complete overview of these functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import os\n",
    "import requests\n",
    "import base64\n",
    "\n",
    "\n",
    "# Encode text values to dummy variables(i.e. [1,0,0],[0,1,0],[0,0,1] for red,green,blue)\n",
    "def encode_text_dummy(df, name):\n",
    "    dummies = pd.get_dummies(df[name])\n",
    "    for x in dummies.columns:\n",
    "        dummy_name = \"{}-{}\".format(name, x)\n",
    "        df[dummy_name] = dummies[x]\n",
    "    df.drop(name, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Encode text values to a single dummy variable.  The new columns (which do not replace the old) will have a 1\n",
    "# at every location where the original column (name) matches each of the target_values.  One column is added for\n",
    "# each target value.\n",
    "def encode_text_single_dummy(df, name, target_values):\n",
    "    for tv in target_values:\n",
    "        l = list(df[name].astype(str))\n",
    "        l = [1 if str(x) == str(tv) else 0 for x in l]\n",
    "        name2 = \"{}-{}\".format(name, tv)\n",
    "        df[name2] = l\n",
    "\n",
    "\n",
    "# Encode text values to indexes(i.e. [1],[2],[3] for red,green,blue).\n",
    "def encode_text_index(df, name):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    df[name] = le.fit_transform(df[name])\n",
    "    return le.classes_\n",
    "\n",
    "\n",
    "# Encode a numeric column as zscores\n",
    "def encode_numeric_zscore(df, name, mean=None, sd=None):\n",
    "    if mean is None:\n",
    "        mean = df[name].mean()\n",
    "\n",
    "    if sd is None:\n",
    "        sd = df[name].std()\n",
    "\n",
    "    df[name] = (df[name] - mean) / sd\n",
    "\n",
    "\n",
    "# Convert all missing values in the specified column to the median\n",
    "def missing_median(df, name):\n",
    "    med = df[name].median()\n",
    "    df[name] = df[name].fillna(med)\n",
    "\n",
    "\n",
    "# Convert all missing values in the specified column to the default\n",
    "def missing_default(df, name, default_value):\n",
    "    df[name] = df[name].fillna(default_value)\n",
    "\n",
    "\n",
    "# Convert a Pandas dataframe to the x,y inputs that TensorFlow needs\n",
    "def to_xy(df, target):\n",
    "    result = []\n",
    "    for x in df.columns:\n",
    "        if x != target:\n",
    "            result.append(x)\n",
    "    # find out the type of the target column.  Is it really this hard? :(\n",
    "    target_type = df[target].dtypes\n",
    "    target_type = target_type[0] if hasattr(target_type, '__iter__') else target_type\n",
    "    # Encode to int for classification, float otherwise. TensorFlow likes 32 bits.\n",
    "    if target_type in (np.int64, np.int32):\n",
    "        # Classification\n",
    "        dummies = pd.get_dummies(df[target])\n",
    "        return df.as_matrix(result).astype(np.float32), dummies.as_matrix().astype(np.float32)\n",
    "    else:\n",
    "        # Regression\n",
    "        return df.as_matrix(result).astype(np.float32), df.as_matrix([target]).astype(np.float32)\n",
    "\n",
    "# Nicely formatted time string\n",
    "def hms_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)\n",
    "\n",
    "\n",
    "# Regression chart.\n",
    "def chart_regression(pred,y,sort=True):\n",
    "    t = pd.DataFrame({'pred' : pred, 'y' : y.flatten()})\n",
    "    if sort:\n",
    "        t.sort_values(by=['y'],inplace=True)\n",
    "    a = plt.plot(t['y'].tolist(),label='expected')\n",
    "    b = plt.plot(t['pred'].tolist(),label='prediction')\n",
    "    plt.ylabel('output')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Remove all rows where the specified column is +/- sd standard deviations\n",
    "def remove_outliers(df, name, sd):\n",
    "    drop_rows = df.index[(np.abs(df[name] - df[name].mean()) >= (sd * df[name].std()))]\n",
    "    df.drop(drop_rows, axis=0, inplace=True)\n",
    "\n",
    "\n",
    "# Encode a column to a range between normalized_low and normalized_high.\n",
    "def encode_numeric_range(df, name, normalized_low=-1, normalized_high=1,\n",
    "                         data_low=None, data_high=None):\n",
    "    if data_low is None:\n",
    "        data_low = min(df[name])\n",
    "        data_high = max(df[name])\n",
    "\n",
    "    df[name] = ((df[name] - data_low) / (data_high - data_low)) \\\n",
    "               * (normalized_high - normalized_low) + normalized_low\n",
    "        \n",
    "# This function submits an assignment.  You can submit an assignment as much as you like, only the final\n",
    "# submission counts.  The paramaters are as follows:\n",
    "# data - Pandas dataframe output.\n",
    "# key - Your student key that was emailed to you.\n",
    "# no - The assignment class number, should be 1 through 1.\n",
    "# source_file - The full path to your Python or IPYNB file.  This must have \"_class1\" as part of its name.  \n",
    "# .             The number must match your assignment number.  For example \"_class2\" for class assignment #2.\n",
    "def submit(data,key,no,source_file=None):\n",
    "    if source_file is None and '__file__' not in globals(): raise Exception('Must specify a filename when a Jupyter notebook.')\n",
    "    if source_file is None: source_file = __file__\n",
    "    suffix = '_class{}'.format(no)\n",
    "    if suffix not in source_file: raise Exception('{} must be part of the filename.'.format(suffix))\n",
    "    with open(source_file, \"rb\") as image_file:\n",
    "        encoded_python = base64.b64encode(image_file.read()).decode('ascii')\n",
    "    ext = os.path.splitext(source_file)[-1].lower()\n",
    "    if ext not in ['.ipynb','.py']: raise Exception(\"Source file is {} must be .py or .ipynb\".format(ext))\n",
    "    r = requests.post(\"https://api.heatonresearch.com/assignment-submit\",\n",
    "        headers={'x-api-key':key}, json={'csv':base64.b64encode(data.to_csv(index=False).encode('ascii')).decode(\"ascii\"),\n",
    "        'assignment': no, 'ext':ext, 'py':encoded_python})\n",
    "    if r.status_code == 200:\n",
    "        print(\"Success: {}\".format(r.text))\n",
    "    else: print(\"Failure: {}\".format(r.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Assignment #7 Sample Code\n",
    "\n",
    "The following code provides a starting point for this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape for dataset: (5, 28, 28)\n",
      "Labels: [0 9 3 1 6]\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras import regularizers\n",
    "from keras.datasets import mnist\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "# This is your student key that I emailed to you at the beginnning of the semester.\n",
    "key = \"UjhQvgInJx71GabltZtqy6O1LdzxtjcE5idLxF3K\"\n",
    "\n",
    "# You must also identify your source file.  (modify for your local setup)\n",
    "# file='/resources/t81_558_deep_learning/assignment_yourname_class1.ipynb'  # IBM Data Science Workbench\n",
    "# file='C:\\\\Users\\\\jeffh\\\\projects\\\\t81_558_deep_learning\\\\t81_558_class1_intro_python.ipynb'  # Windows\n",
    "file='/Users/jwalker/git/t81_558_deep_learning/assignments/assignment_jwalker_class7.ipynb'  # Mac/Linux\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# The following lines are just an example, and must be replaced with your solution.\n",
    "ids = [10,20,30,40,50]\n",
    "#pred2 = [0,0,0,0,0]\n",
    "#y_comp = [0,9,3,1,6]\n",
    "\n",
    "#small_x = x_test[ids]\n",
    "#y_comp = y_test[ids]\n",
    "#print(\"Shape for dataset: {}\".format(small_x.shape))\n",
    "#print(\"Labels: {}\".format(y_comp))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "Training samples: 60000\n",
      "Test samples: 10000\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 12\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(\"Training samples: {}\".format(x_train.shape[0]))\n",
    "print(\"Test samples: {}\".format(x_test.shape[0]))\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      " - 241s - loss: 0.3366 - acc: 0.8967 - val_loss: 0.0788 - val_acc: 0.9760\n",
      "Epoch 2/12\n",
      " - 239s - loss: 0.1119 - acc: 0.9666 - val_loss: 0.0516 - val_acc: 0.9831\n",
      "Epoch 3/12\n",
      " - 251s - loss: 0.0825 - acc: 0.9759 - val_loss: 0.0427 - val_acc: 0.9862\n",
      "Epoch 4/12\n",
      " - 275s - loss: 0.0696 - acc: 0.9790 - val_loss: 0.0388 - val_acc: 0.9877\n",
      "Epoch 5/12\n",
      " - 259s - loss: 0.0610 - acc: 0.9819 - val_loss: 0.0361 - val_acc: 0.9873\n",
      "Epoch 6/12\n",
      " - 247s - loss: 0.0538 - acc: 0.9836 - val_loss: 0.0299 - val_acc: 0.9896\n",
      "Epoch 7/12\n",
      " - 247s - loss: 0.0513 - acc: 0.9848 - val_loss: 0.0305 - val_acc: 0.9894\n",
      "Epoch 8/12\n",
      " - 255s - loss: 0.0458 - acc: 0.9864 - val_loss: 0.0305 - val_acc: 0.9897\n",
      "Epoch 9/12\n",
      " - 238s - loss: 0.0430 - acc: 0.9874 - val_loss: 0.0288 - val_acc: 0.9903\n",
      "Epoch 10/12\n",
      " - 236s - loss: 0.0416 - acc: 0.9879 - val_loss: 0.0285 - val_acc: 0.9903\n",
      "Epoch 11/12\n",
      " - 237s - loss: 0.0390 - acc: 0.9886 - val_loss: 0.0296 - val_acc: 0.9889\n",
      "Epoch 12/12\n",
      " - 235s - loss: 0.0372 - acc: 0.9887 - val_loss: 0.0295 - val_acc: 0.9902\n",
      "Test loss: 0.029498770529893227\n",
      "Test accuracy: 0.9902\n",
      "Elapsed time: 0:49:31.96\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=2,\n",
    "          validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss: {}'.format(score[0]))\n",
    "print('Test accuracy: {}'.format(score[1]))\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"Elapsed time: {}\".format(hms_string(elapsed_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "small_x = x_test[ids]\n",
    "y_comp = y_test[ids]\n",
    "\n",
    "y_comp = np.argmax(y_comp,axis=1)\n",
    "\n",
    "pred = model.predict(small_x)\n",
    "\n",
    "pred2 = np.argmax(pred,axis=1)\n",
    "\n",
    "score = metrics.accuracy_score(y_comp, pred2)\n",
    "\n",
    "print('Accuracy: {}'.format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submit_df = pd.DataFrame({'id':ids,'pred':pred2,'y':y_comp})\n",
    "\n",
    "# Save a copy if you like\n",
    "submit_df.to_csv('7.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: Submitted Assignment #7 for walker-jason:\n",
      "This is your first submission of this assignment.\n",
      "No warnings on your data. You will probably do well, but no guarantee. :-)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Submit the assignment\n",
    "submit(source_file=file,data=submit_df,key=key,no=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
